Module for intel-oneAPI 2021.4.0 loaded.
Module for gcc 10.2:
(GNU C/C++/FORTRAN compilers v10.2.0) - loaded.
Module for OpenMPI 4.1.0-gcc10.2-infiniband loaded.

     Program PWSCF v.7.0 starts on 15Mar2022 at 10:18:53 

     This program is part of the open-source Quantum ESPRESSO suite
     for quantum simulation of materials; please cite
         "P. Giannozzi et al., J. Phys.:Condens. Matter 21 395502 (2009);
         "P. Giannozzi et al., J. Phys.:Condens. Matter 29 465901 (2017);
         "P. Giannozzi et al., J. Chem. Phys. 152 154105 (2020);
          URL http://www.quantum-espresso.org", 
     in publications or presentations arising from this work. More details at
     http://www.quantum-espresso.org/quote

     Parallel version (MPI), running on    36 processors

     MPI processes distributed on     2 nodes
     K-points division:     npool     =       2
     R & G space division:  proc/nbgrp/npool/nimage =      18
     42989 MiB available memory on the printing compute node when the environment starts

     Reading input from in_files/silicon_bench_nk_2_n_procs_36.scf

     Current dimensions of program PWSCF are:
     Max number of different atomic species (ntypx) = 10
     Max number of k-points (npk) =  40000
     Max angular momentum in pseudopotentials (lmaxx) =  4

     Subspace diagonalization in iterative solution of the eigenvalue problem:
     one sub-group per band group will be used
     scalapack distributed-memory algorithm (size of sub-group:  3*  3 procs)


     Parallelization info
     --------------------
     sticks:   dense  smooth     PW     G-vecs:    dense   smooth      PW
     Min          55      55     16                 1158     1158     191
     Max          56      56     17                 1161     1161     195
     Sum        1003    1003    301                20875    20875    3479

     Using Slab Decomposition



     bravais-lattice index     =            2
     lattice parameter (alat)  =      10.2000  a.u.
     unit-cell volume          =     265.3020 (a.u.)^3
     number of atoms/cell      =            2
     number of atomic types    =            1
     number of electrons       =         8.00
     number of Kohn-Sham states=            4
     kinetic-energy cutoff     =      70.0000  Ry
     charge density cutoff     =     280.0000  Ry
     scf convergence threshold =      1.0E-12
     mixing beta               =       0.7000
     number of iterations used =            8  plain     mixing
     Exchange-correlation= PBE
                           (   1   4   3   4   0   0   0)

     celldm(1)=  10.200000  celldm(2)=   0.000000  celldm(3)=   0.000000
     celldm(4)=   0.000000  celldm(5)=   0.000000  celldm(6)=   0.000000

     crystal axes: (cart. coord. in units of alat)
               a(1) = (  -0.500000   0.000000   0.500000 )  
               a(2) = (   0.000000   0.500000   0.500000 )  
               a(3) = (  -0.500000   0.500000   0.000000 )  

     reciprocal axes: (cart. coord. in units 2 pi/alat)
               b(1) = ( -1.000000 -1.000000  1.000000 )  
               b(2) = (  1.000000  1.000000  1.000000 )  
               b(3) = ( -1.000000  1.000000 -1.000000 )  


     PseudoPot. # 1 for Si read from file:
     ./pseudos/Si_pbe_v0.4.1.nc.sr.upf
     MD5 check sum: 02fab3f35e82123ef5bf1cb05d5b1a5e
     Pseudo is Norm-conserving + core correction, Zval =  4.0
     Generated using ONCVPSP code by D. R. Hamann
     Using radial grid of 1510 points,  6 beta functions with: 
                l(1) =   0
                l(2) =   0
                l(3) =   1
                l(4) =   1
                l(5) =   2
                l(6) =   2

     atomic species   valence    mass     pseudopotential
        Si             4.00    28.08600     Si( 1.00)

     48 Sym. Ops., with inversion, found (24 have fractional translation)



   Cartesian axes

     site n.     atom                  positions (alat units)
         1           Si  tau(   1) = (   0.0000000   0.0000000   0.0000000  )
         2           Si  tau(   2) = (   0.2500000   0.2500000   0.2500000  )

     number of k points=  1661

     Number of k-points >= 100: set verbosity='high' to print them.

     Dense  grid:    20875 G-vectors     FFT dimensions: (  40,  40,  40)

     Estimated max dynamical RAM per process >       9.64 MB

     Estimated total dynamical RAM >     341.81 MB

     Initial potential from superposition of free atoms

     starting charge       7.9988, renormalised to       8.0000
     Starting wfcs are    8 randomized atomic wfcs

     total cpu time spent up to now is       27.8 secs

     Self-consistent Calculation

     iteration #  1     ecut=    70.00 Ry     beta= 0.70
     Davidson diagonalization with overlap
     ethr =  1.00E-02,  avg # of iterations =  2.0

     Threshold (ethr) on eigenvalues was too large:
     Diagonalizing with lowered threshold

     Davidson diagonalization with overlap
     ethr =  5.86E-04,  avg # of iterations =  2.0

     total cpu time spent up to now is       89.7 secs

     total energy              =     -16.92149471 Ry
     estimated scf accuracy    <       0.05528617 Ry

     iteration #  2     ecut=    70.00 Ry     beta= 0.70
     Davidson diagonalization with overlap
     ethr =  6.91E-04,  avg # of iterations =  1.0

     total cpu time spent up to now is      113.8 secs

     total energy              =     -16.92319818 Ry
     estimated scf accuracy    <       0.00291059 Ry

     iteration #  3     ecut=    70.00 Ry     beta= 0.70
     Davidson diagonalization with overlap
     c_bands:  2 eigenvalues not converged

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Error in routine  cdiaghg (8):
      problems computing cholesky
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

     stopping ...
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 8.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[1647336052.387396] [idefix881:1977 :0]           sock.c:451  UCX  ERROR recv(fd=29) failed: Connection reset by peer
[idefix882:21927] 8 more processes have sent help message help-mpi-api.txt / mpi-abort
[idefix882:21927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Command exited with non-zero status 8
real 122.32
user 1462.89
sys 922.85
