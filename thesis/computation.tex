\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Computational Basics\label{chap:computation}}
%\epigraph{It's a secret to everybody.}{a Moblin in \textit{The Legend of Zelda}}

\section{Parallel computing\label{sec:parallel_computing}}

The following section will give an overview of the technical aspects of running computer code (such as \QE) on massively parallel computing environments (such as the PHYSnet compute cluster).
The information presented can be found in any textbook on parallel or high-perfomance computing \cite{hager_introduction_2010}.

\subsection{On scalabilty}

In scientific computing, one can identify two distinct reasons to distribute workloads to multiple processors:
\begin{itemize}
    \item The execution time on a single core is not sufficient. The definition of sufficient is dependent on the specific task and can range from \enquote{over lunch} to \enquote{multiple weeks}
    \item The memory requirements grow outside the capabilities of a single core
\end{itemize}
In order to judge how well a task can parallelized, usually some sort of scalabilty metric is employed, for example:
\begin{itemize}
    \item How fast can a problem be solved with \(N\) processors instead of one?
    \item What kind of bigger problem (finer resolution, more particles, etc.) can be solved with \(N\) processors?
    \item How much of the resources is used for solving the problem?
\end{itemize}
The speedup by using \(N\) workers to solve a problem instead of one is defined as \(S = \frac{T_1}{T_N}\), where \(T_1\) is the execution time on a single processor and \(T_N\) is the execution time on \(N\) processors.
In the ideal case, where all the work can be perfectly distributed among the processors, all processors need the same time for their respective workloads and don't have to wait for others processors to finish their workload to continue, the execution time on \(N\) processors would be \(\frac{T_1}{N}\), so the speedup would be \(S = \frac{T_1}{\frac{T_1}{N}} = N\).

In reality, there are many factors either limiting or in some cases supporting parallel code scalability. Limiting factors include:
\begin{itemize}
    \item \emph{Algorithmic limitations}: when parts of a calculation are mutually dependent on each other, the calculation cannot be fully parallelized
    \item \emph{Bottlenecks}: in any computer system exist resources which are shared between processor cores with limitations on parallel access. This serializes the execution by requiring cores to wait for others to complete the task which uses the shared resources in question
    \item \emph{Startup Overhead}: introducing parallelization into a programm necessarily introduces an overhead, e.g. for distributing data across all the processors
    \item \emph{Communication}: often solving a problem requires communication between different cores (e.g. exchange of interim results after a step of the calculation). Communication can be implemented very effectively, but can still introduce a big prize in computation time
\end{itemize}
On the other hand, faster parallel code execution can come from:
\begin{itemize}
    \item \emph{Better caching}: when the data the program is working with is distributed among processors (assuming constant problem size), it may enable the data to be stored in faster cache memory. Modern computers typically have three layers of cache memory, with level 1 cache being the smallest and fastest and level 3 being the largest and slowest, so smaller data chunks per processor can lead to the data not being stored in main memory, but completely in cache or in a faster cache level
\end{itemize}

\section{\QE}\label{sec:qe}

\end{document}
