\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Computational Details\label{ch:computation}}
%\epigraph{It's a secret to everybody.}{a Moblin in \textit{The Legend of Zelda}}

\section{Parallel computing\label{sec:parallel_computing}}

The following section will give an overview of the technical aspects of running computer code (such as \QE) on massively parallel computing environments (such as the PHYSnet compute cluster).
The information presented can be found in any textbook on parallel or high-perfomance computing \cite{hager_introduction_2010}.

\todo{functional/data parallelism?}

\subsection{On scalabilty}

In scientific computing, one can identify two distinct reasons to distribute workloads to multiple processors:
\begin{itemize}
    \item The execution time on a single core is not sufficient. The definition of sufficient is dependent on the specific task and can range from \enquote{over lunch} to \enquote{multiple weeks}
    \item The memory requirements grow outside the capabilities of a single core
\end{itemize}
In order to judge how well a task can parallelized, usually some sort of scalabilty metric is employed, for example:
\begin{itemize}
    \item How fast can a problem be solved with \(N\) processors instead of one?
    \item What kind of bigger problem (finer resolution, more particles, etc.) can be solved with \(N\) processors?
    \item How much of the resources is used for solving the problem?
\end{itemize}
The speedup by using \(N\) workers to solve a problem instead of one is defined as \(S = \frac{T_1}{T_N}\), where \(T_1\) is the execution time on a single processor and \(T_N\) is the execution time on \(N\) processors.
In the ideal case, where all the work can be perfectly distributed among the processors, all processors need the same time for their respective workloads and don't have to wait for others processors to finish their workload to continue, the execution time on \(N\) processors would be \(\frac{T_1}{N}\), so the speedup would be \(S = \frac{T_1}{\frac{T_1}{N}} = N\).

In reality, there are many factors either limiting or in some cases supporting parallel code scalability. Limiting factors include:
\begin{itemize}
    \item \emph{Algorithmic limitations}: when parts of a calculation are mutually dependent on each other, the calculation cannot be fully parallelized
    \item \emph{Bottlenecks}: in any computer system exist resources which are shared between processor cores with limitations on parallel access. This serializes the execution by requiring cores to wait for others to complete the task which uses the shared resources in question
    \item \emph{Startup Overhead}: introducing parallelization into a programm necessarily introduces an overhead, e.g. for distributing data across all the processors
    \item \emph{Communication}: often solving a problem requires communication between different cores (e.g. exchange of interim results after a step of the calculation). Communication can be implemented very effectively, but can still introduce a big prize in computation time
\end{itemize}
On the other hand, faster parallel code execution can come from:
\begin{itemize}
    \item \emph{Better caching}: when the data the program is working with is distributed among processors (assuming constant problem size), it may enable the data to be stored in faster cache memory. Modern computers typically have three layers of cache memory, with level 1 cache being the smallest and fastest and level 3 being the largest and slowest, so smaller data chunks per processor can lead to the data not being stored in main memory, but completely in cache or in a faster cache level
\end{itemize}

\todo{more details strong/weak scaling?}

\section{\QE}\label{sec:qe}

\QE (opEn-Source Package for Research in Electronic Structure, Simulation, and Optimization) \cite{giannozzi_quantum_2009,giannozzi_advanced_2017} is a collection of packages implementing (among others) the techniques described in sec. \ref{sec:theory_dft} and \ref{seq:theory_dfpt} to calculate electronic structure properties as well as phonon frequencies and eigenvectors.

\subsection{Compilation of \QE}\label{sub:qe_compilation}

As the core of this thesis is an in depth examination of the \QE software and ways its performance can be optimized, a discussion of the way is is compiled is needed.
The information in this section is taken from the \QE 7.0 user guide \cite{noauthor_quantum_nodate}.

The \QE distribution is packaged with everything needed for simple, non-parallel execution, the only additional software needed are a minimal Unix environment (a shell like \texttt{bash} or \texttt{sh} as well as the utilities \texttt{make} \texttt{awk} and \texttt{sed}) and a Fortran compiler compliant with the F2008 standard.
For parallel execution, also \gls{mpi} libraries and an \gls{mpi} aware compiler need to be provided.

\QE needs three external mathematical libraries, \gls{blas} and \gls{lapack} for linear algebra as well as an implementation of \gls{fft} for fourier transforms.
In order to make the installation as easy as possible, \QE comes with a publicly available reference implementation of the \gls{blas} routines, the publicly available \gls{lapack} package and an older version of FFTW (Fastest Fourier Transform in the West, an open source implementation of \gls{fft}).
Even though these libraries are already optimized in terms of the algorithms implemented, usage of libraries implementing the same routines which can use more specific CPU optimizations significantly improves performance (e.g. libraries provided by Intel can use CPU optimizations not present on AMD processors).

On the PHYSnet cluster, a variety of software packages are available as modules.
The benchmark in this thesis were made using the following modules:
\begin{itemize}
    \item \texttt{openmpi/4.1.1.gcc10.2-infiniband}: \gls{openmpi} 4.1.0 (implies usage of \QE provided \gls{blas}/\gls{lapack})
    \item \texttt{openmpi/4.1.1.gcc10.2-infiniband openblas/0.3.20}: \gls{openmpi} 4.1.0 and \gls{openblas} 0.3.20
    \item \texttt{scalapack/2.2.0}: \gls{openmpi} 4.1.0, \gls{openblas} 0.3.20 and \gls{scalapack} 2.2.0
    \item \texttt{intel/oneAPI-2021.4}: \gls{oneapi} 2021.4
\end{itemize}

\QE offers an configuration script to automatically find all required libraries.
As the default options of the \texttt{configure} script work well in the use case of this thesis, all compilations were made using the minimal commands
\begin{verbatim}
    module load <module names>
    ./configure --with-scalapack=no|yes|intel
\end{verbatim}
with the scalapack options yes (when using \texttt{scalapack/2.2.0}), intel (when using \\ \texttt{intel/oneAPI-2021.4}) and no otherwise.


\todo{what to look out for in configure output: when are the internal copies of blas and lapack used?}


\subsection{Parallelization capabilities offered by \QE}\label{sub:qe_parallelization}

\begin{wrapfigure}{l}{0.5\textwidth}
\centering
\begin{tikzpicture}[
    squarednode/.style={rectangle, draw=purple!60, fill=purple!5, thick, minimum size=5mm, text centered, text width=3cm,},
    node distance=0.5cm]
    %Nodes
    \node[squarednode]      (init)          {initial \(n(\vb{r})\)};
    \node[squarednode]      (step_1)    [below= of init]  {calculate potentials \(v_H [n(\vb{r})]\), \(v_{XC} [n(\vb{r})]\)};
    \node[squarednode]      (step_2)    [below= of step_1]      {fourier transform potentials};
    \node[squarednode]      (step_3)    [below= of step_2]      {solve \acrshort{kohn_sham} equations in reciprocal space};
    \node[squarednode]      (step_4)    [below= of step_3]      {fourier transform wavefunctions};
    \node[squarednode]      (step_5)    [below= of step_4]      {calculate \(n(\vb{r})\) and other physical properties (\(E [n(\vb{r})]\))};
    \node[squarednode]      (done)      [below= of step_5]      {done if change in \(E\) is small enough};
    
    %Lines
    \draw[->] (init.south) -- (step_1.north);
    \draw[->] (step_1.south) -- (step_2.north);
    \draw[->] (step_2.south) -- (step_3.north);
    \draw[->] (step_3.south) -- (step_4.north);
    \draw[->] (step_4.south) -- (step_5.north);
    \draw[->] (step_5.east) to [out=30,in=-30] (step_1.east);
    \draw[->] (step_5.south) -- (done.north);
\end{tikzpicture}
\label{fig:diagram_scf_calculations}
\caption{Flowchart of an algorithm to solve the \acrshort{kohn_sham} equations}
\end{wrapfigure}

Fig. \ref{fig:diagram_scf_calculations} shows a possible approach to solving the \acrshort{kohn_sham} equations.
This opens a few possibilities for parallelization of calculations: first of all, the orbitals in the plane wave basis set as well as charges and densities can be distributed among processors.
This distribution of data mainly works around memory constraints, as using more processors lowers the memory requirement for every single processor.
Going further, \QE automatically parallelizes all linear algebra operations on real space/reciprocal grid.
The price to pay for this parallelization is the need for communication between processors: as an example, fourier transforms always need to collect and distribute contributions from both representation in order to transform between them.

To remedy this problem, \todo{k point parallelization}

In another level of parallelization, \QE can use \gls{scalapack} to parallelize among other things the iterative orthonormalization of \acrshort{kohn_sham} states.
This parallelization level is called \emph{linear algebra parallelization} and is controlled by the CLI parameter \texttt{-nd <number of processors in linear algebra group}.
Importantly, this parameter sets the size for the linear algebra group in every k point processor pool, so the number of processors in the linear algebra group has to be smaller than the number of processors in one pool.
Furthermore, the arrays on which the calculations are performed on are distributed in a 2D grid among processors, so the number of processors in the linear algebra has to be \(n^2\), where \(n\) is an integer.

\todo{short conclusion: which parameters, how can they be chose?}

\section{Hardware configuration of the PHYSnet cluster}

\todo{is this good here?}

\end{document}
