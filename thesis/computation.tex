\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Computational Details\label{ch:computation}}

\section{Parallel computing and scalability\label{sec:parallel_computing}}

The following section will give an overview of the technical aspects of running computer code, such as \QE, on massively parallel computing environments.
The information presented in this section follows closely the textbook on high-performance computing by Hager and Wellein \cite{hager_introduction_2010}.

In scientific computing, one can identify two distinct reasons for distributing workload to multiple processors:
\begin{itemize}
    \item The execution time on a single core is not sufficient. The definition of sufficient is dependent on the specific task and can range from \enquote{over lunch} to \enquote{multiple weeks}.
    \item The memory requirements exceed the capabilities of a single core.
\end{itemize}

Parallelization of a task across multiple processors can be distinguished into two ways:
\begin{description}
    \item[Single Program Multiple Data (SPMD)] Every processor runs the same program, with data distributed among processors.
    \item[Multiple Program Multiple Data (MPMD)] Every processor runs a different function, for example in a pipelining process where multiple consequent operations on the input data need to be done and data comes in chunks, so every step of the pipeline can run independent of the others.
\end{description}
The typical case in physics is SPMD.
For instance, many calculations require diagonalization of matrices, which can be iteratively done with algorithms requiring only knowledge of the data of the nearest neighbors for every matrix element in every step.
This enables parallelization as the whole matrix can be distributed and communication is only required at the bordering regions for exchange of data in every iteration step.
In the case where the iteration step in one region is faster than in another, waiting times are introduced, as the next iteration step can only be done after data exchange with the neighboring regions.

In order to judge how well a task can be parallelized, a scalability metric is employed, for example:
\begin{itemize}
    \item How fast can a problem be solved with \(N\) processors instead of one?
    \item What kind of bigger problem (finer resolution, more particles, etc.) can be solved with \(N\) processors?
    \item How efficiently are the resources utilized?
\end{itemize}
In this thesis, the main concern is speeding up the execution of extensively expensive calculations with a fixed problem size, so the first metric will be used to judge the quality of parallelization.
This metric is called speedup and is defined as 
\begin{equation}\label{eq:definition_speedup}
    S \coloneqq \frac{T_1}{T_N}\,, 
\end{equation}
where \(T_1\) is the execution time on a single processor and \(T_N\) is the execution time on \(N\) processors.
In the ideal case, where all the work can be perfectly distributed among the processors and all processors need the same time for their respective workload, the execution time on \(N\) processors would be \(\nicefrac{T_1}{N}\), so inserting this into eq. \ref{eq:definition_speedup} gives a speedup of
\begin{equation}\label{eq:ideal_speedup}
    S = \frac{T_1}{\frac{T_1}{N}} = N\,.
\end{equation}

In reality, there are many factors either limiting or in some cases supporting parallel code scalability. Limiting factors include:
\begin{description}
    \item[Algorithmic limitations] When parts of a calculation are mutually dependent on each other, the calculation cannot be fully parallelized.
    \item[Bottlenecks] In any computer system exist resources which are shared between processor cores with limitations on parallel access. This serializes the execution by requiring cores to wait for others to complete the task which uses the shared resources in question.
    \item[Startup Overhead] Introducing parallelization into a program necessarily introduces an overhead, e.g. for distributing data across all the processors.
    \item[Communication] Often solving a problem requires communication between different cores (e.g. exchange of interim results after a step of the calculation). Communication can be implemented very effectively, but can still introduce a big prize in computation time.
\end{description}
On the other hand, \emph{better caching} can lead to better scaling than \(S = N\): as optimal performance per core is achieved when all the data can be kept in cache, reducing the data size per processor by distributing data among more processors can lead to each individual processor being faster than in the single core case.

A simple ansatz for modeling speedup with these limitations in mind was first derived by Gene Amdahl \cite{amdahl_validity_1967}.
Assuming the work that needs to be done is split into a part which cannot be parallelized \(s\) and a part which can be parallelized ideally \(p\), serial time can be normalized to 1:
\begin{equation}
    T_1 = s + p = 1
\end{equation}
The time for solving the problem on \(N\) processors is then
\begin{equation}
    T_N = s + \frac{p}{N}\,.
\end{equation}
The speedup is now
\begin{equation}\label{eq:amdahls_law}
    S = \frac{T_s}{T_p} = \frac{1}{s + \frac{p}{N}} = \frac{1}{s + \frac{1 - s}{N}}\,.
\end{equation}
This equation is called \emph{Amdahl's law}.
It shows that even for \(N \to \infty\), the speedup has an upper bound of \(\frac{1}{s}\).
Furthermore, the value of \(s\) determines the range of processors where the speedup is close to the ideal case, as shown in fig. \ref{fig:amdahl}.
It shows that for a bigger value of \(s\), not only leads to the speedup saturating at a smaller constant, but also differing significantly from the ideal case even for a small number of processors.
For \(s = 0.01\), a speedup of around 16 for 20 processors used can be deemed acceptable in terms of how efficient the computing resources are used, whereas even using more than 6 processors in the case of \(s = 0.2\) is not.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{amdahl.pdf}
    \caption{Speedup modeled by Amdahl's law for different portions of strictly serial workload}
    \label{fig:amdahl}
\end{figure}

The weakness of Amdahl's law lies in the simplicity of it, as the different factors limiting parallelization are generally not independent of \(N\).
Communication overhead would need to be accounted for with some kind of function \(c(N)\) with the form depending on many factors like speed and bandwidth of the communication hardware or the way the data is distributed.

With just Amdahl's law, decomposition into the several factors limiting parallelization is not possible, so an assessment of how calculations can be improved in detail is also not possible.
Regardless, Amdahl's law explains in simple ways how speedup can differ from the ideal case and can also be reasonable accurate when the costs for communication don't depend strongly on \(N\).


\section{\QE}\label{sec:qe}

\QE (opEn-Source Package for Research in Electronic Structure, Simulation, and Optimization) \cite{giannozzi_quantum_2009,giannozzi_advanced_2017} is a collection of packages implementing (among others) the techniques described in sec. \ref{sec:theory_dft} and \ref{sec:theory_dfpt} to calculate electronic structure properties (module \texttt{PWscf}) as well as phonon frequencies and eigenmodes (module \texttt{PHonon}).

\subsection{Compilation of \QE}\label{sub:qe_compilation}

As motivated above, the main goal of this thesis is an in-depth analysis of the QE software with respect to performance in terms of used computation resources.
The choice and availability of different compilers will significantly influence this.
Therefore, a short overview is presented here.
The information in this section is taken from the \QE 7.0 user guide \cite{noauthor_quantum_nodate}.

The \QE distribution is packaged with everything needed for simple, non-parallel execution, the only additional software needed are a minimal Unix environment (a shell like \texttt{bash} or \texttt{sh} as well as the utilities \texttt{make} \texttt{awk} and \texttt{sed}) and a Fortran compiler compliant with the F2008 standard.
For parallel execution, also \gls{mpi} libraries and an \gls{mpi} aware compiler need to be provided.

\QE needs three external mathematical libraries, \gls{blas} and \gls{lapack} for linear-algebra as well as an implementation of \gls{fft} for Fourier transforms.
In order to make the installation as easy as possible, \QE comes with a publicly available reference implementation of the \gls{blas} routines, the publicly available \gls{lapack} package and an older version of FFTW (Fastest Fourier Transform in the West, an open source implementation of \gls{fft}).
Even though these libraries are already optimized in terms of the algorithms implemented, usage of libraries implementing the same routines which can use more specific CPU optimizations might improve performance, e.g. libraries included in \gls{oneapi}, which are optimized for use on Intel CPUs.

On the PHYSnet cluster, a variety of software packages are available as modules.
The benchmarks in this thesis were made using the following module combinations:
\begin{itemize}
    \item \texttt{openmpi/4.1.1.gcc10.2-infiniband}: \gls{openmpi} 4.1.0 (implies usage of \QE provided \gls{blas}/\gls{lapack})
    \item \texttt{openmpi/4.1.1.gcc10.2-infiniband openblas/0.3.20}: \gls{openmpi} 4.1.0 and \gls{openblas} 0.3.20
    \item \texttt{scalapack/2.2.0}: \gls{openmpi} 4.1.0, \gls{openblas} 0.3.20 and \gls{scalapack} 2.2.0
    \item \texttt{intel/oneAPI-2021.4}: \gls{oneapi} 2021.4
\end{itemize}

\QE offers a configuration script to automatically find all required libraries.
As the default options of the \texttt{configure} script work well in the use case of this thesis, all compilations were made using the minimal commands
\begin{verbatim}
    module load <module names>
    ./configure --with-scalapack=no|yes|intel
\end{verbatim}
with the scalapack options yes (when using \texttt{scalapack/2.2.0}), intel (when using \\ \texttt{intel/oneAPI-2021.4}) and no otherwise.

The output of the configuration script gives information about the detected libraries.
In the following output, the Intel \gls{oneapi} package was loaded, so \gls{blas} and \gls{scalapack} libraries from that package will be used, whereas the included \gls{fft} library will be used:
\begin{verbatim}
    The following libraries have been found:
    BLAS_LIBS=  -lmkl_intel_lp64  -lmkl_sequential -lmkl_core
    LAPACK_LIBS=
    SCALAPACK_LIBS=-lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
    FFT_LIBS= 
\end{verbatim}

\subsection{Parallelization capabilities implemented in \QE}\label{sub:qe_parallelization}

\QE is intended to be used in parallel environments and as such offers possibilities to manage how the work is parallelized.
This section introduces the parallelization capabilities of the \texttt{PWscf} and \texttt{PHonon} modules and explores how they potentially affect the scaling behavior of \QE. The information in this section stems from the user guides for the two modules \cite{noauthor_pwscf_nodate, noauthor_phonon_nodate}

\begin{figure}[htb!]
%\centering
\begin{tikzpicture}[
    squarednode/.style={rectangle, draw=purple!60, fill=purple!5, thick, minimum size=5mm, text centered, text width=8cm,},
    node distance=0.5cm]
    \tikzset{every node/.style={inner sep=8pt}}
    %Nodes
    \node[squarednode]      (init)          {Initial \(n(\vb*{r})\)};
    \node[squarednode]      (step_1)    [below= of init]  {Calculate \(v_{\mathrm{H}} [n(\vb*{r})]\) and \(v_{\mathrm{XC}} [n(\vb*{r})]\)};
    \node[squarednode]      (step_2)    [below= of step_1]      {Fourier transform potentials};
    \node[squarednode]      (step_3)    [below= of step_2]      {Solve \acrshort{kohn_sham} equations in reciprocal space};
    \node[squarednode]      (step_4)    [below= of step_3]      {Fourier transform wave functions};
    \node[squarednode]      (step_5)    [below= of step_4]      {Calculate \(n(\vb*{r})\) and \(E [n(\vb*{r})]\)};
    \node[squarednode]      (done)      [below= of step_5]      {Done if change in \(E\) is small enough};
    
    %Lines
    \draw[->] (init.south) -- (step_1.north);
    \draw[->] (step_1.south) -- (step_2.north);
    \draw[->] (step_2.south) -- (step_3.north);
    \draw[->] (step_3.south) -- (step_4.north);
    \draw[->] (step_4.south) -- (step_5.north);
    \draw[->] (step_5.east) to [out=30,in=-30] (step_1.east);
    \draw[->] (step_5.south) -- (done.north);
\end{tikzpicture}
\caption{Flowchart of an iterative algorithm to solve the \acrshort{kohn_sham} equations with the use of Fourier transform. As the density \(n(\vb*{r})\) determines again the potentials going into the \acrshort{kohn_sham} equations, steps 2-5 are run until self-consistency is reached.}
\label{fig:diagram-scf-calculations}
\end{figure}
Fig. \ref{fig:diagram-scf-calculations} shows a possible approach to solving the \acrshort{kohn_sham} equations.
The algorithm is taken from the textbook by Martin \cite{martin_electronic_2004}, with the Fourier transform steps added to establish in which representation every calculation step is run.

A few possibilities for parallelization of calculations can be derived from that.
First of all, the real and reciprocal space are discretized to allow for numerical treatment, these grids can be distributed among processors, meaning the wave functions in the plane-wave basis set as well as charges and densities.
This distribution of data mainly works around memory constraints, as using more processors lowers the memory requirement for every single processor.
Going further, \QE automatically parallelizes all linear-algebra operations on this real space/reciprocal grid.
The price to pay for this parallelization is the need for communication between processors: as an example, Fourier transforms always need to collect and distribute contributions from and to the whole reciprocal/real grid in order to transform between them.
This kind of parallelization is called \emph{PW (plane-wave)} or \emph{R\&G (real \& reciprocal)} parallelization. 

As discussed in sec. \ref{sub:theory_basis_set}, the density in the plane-wave basis set is a sum over different \(k\) points, where the calculation for these are independent of each other until calculating the density \(n( \vb*{r})\).
In \QE this is implemented such that a separation of the total number of processors into smaller pools, each doing the calculations for a set of \(k\) points is possible.
This is called \emph{\(k\)-point parallelization}.
The CLI parameter \texttt{-nk <number of pools>} determines how many pools the total number of processor \(N\) is split into.
Hence, the resulting number of processors in one pool is \(\nicefrac{N}{N_k}\).
Within one \(k\)-point processor pool, the PW parallelization with its heavy communication is automatically applied.

On a level of parallelization independent of that, \QE can use \gls{scalapack} to parallelize (among other things) the iterative orthonormalization of \acrshort{kohn_sham} states.
This parallelization level is called \emph{linear-algebra parallelization} and is controlled by the CLI parameter \texttt{-nd <number of processors in linear-algebra group>}.
Importantly, this parameter sets the size for the linear-algebra group in every \(k\)-point processor pool, so the number of processors in the linear-algebra group has to be smaller than the number of processors in one pool.
Furthermore, the arrays on which the calculations are performed on are distributed in a 2D grid among processors.
This means that the number of processors in the linear-algebra group has to be a square number.

In the case of the \texttt{PHonon} module, the representation of states in a plane-wave basis set stays the same, so all three parallelization schemes mentioned for the \texttt{PWscf} module can also be employed.
Furthermore, as calculations for two phonon wave vectors \(\vb*{q}, \vb*{q}^{\prime}\) are not coupled (as different wave vectors lead to different perturbations and as such independent self-consistent equations), they can be split up into images.
The concept of image parallelization in \QE is actually more general than just for phonon calculations, as other kinds of independent iterative calculations can also be run with image parallelization.
The parameter controlling image parallelization is \texttt{ni <number of images>}.
Following this, the number of processors in one \(k\)-point pool is then given by \(N / N_i / N_k\), if image and k-point parallelization is applied, where \(N_\mathrm{i}\) denotes the number of images.

\subsection{Evaluating the scalability of \QE calculations}\label{sub:scalability_qe}

In the \QE output, a time report is printed at the end.
This time report includes \gls{cpu_time} and \gls{wall_time}.
Three different metrics of scalability can be calculated from this:
\begin{itemize}
    \item runtime: absolute runtime (\gls{wall_time}) of the compute job
    \item speedup: runtime on \(N\) processors divided by runtime on a single core
    \item \gls{wait_time}: percentage of \gls{wall_time} not used by \QE process, so writing to disk, waiting for IO devices or other processes, etc. (calculated as (\gls{wall_time} - \gls{cpu_time}) / \gls{wall_time})
\end{itemize}
For analysis mainly speedup will be used to evaluate the scalability of \QE calculation.
It makes comparing the scaling of calculations with different absolute runtimes easy: as discussed in sec. \ref{sec:parallel_computing}, optimal scaling is achieved when the speedup has a slope of one, independent of the runtime.

Regardless, the other two parameters should also always be considered.
In the end, absolute runtime is the most important factor and should govern the decision of how much computational resources should be used for solving a particular problem.
For instance, a problem with a single core runtime of \(\SI{600}{\s}\), a speedup of 100 would mean a runtime of \(\SI{6}{\s}\), whereas a speedup of 200 would mean a runtime of \(\SI{3}{\s}\).
Even with optimal scaling, the 100 processors needed for the speedup of 200 could be considered wasted for just \(\SI{3}{s}\) of saved time.
On the other hand, for a problem with a single core runtime of \(\SI{2400}{\hour}\), the difference between a speedup of 100 (runtime \(\SI{24}{\hour}\)) and 200 (runtime \(\SI{12}{\hour}\)) is the difference between waiting a whole day for the calculation and being able to let the job run overnight and continue in the morning, so using 100 more processors for that can be considered a good use of resources.

As for the \gls{wait_time}, this metric can be used to separate the different factors of poor parallelization discussed in \ref{sec:parallel_computing}.
Startup overhead is easy to identify, as this should be a small, near constant percentage of the absolute runtime.
This of course can vary depending on how complex data distribution is, but there should at least not be a strong dependence on the number of processors, as only a small amount of communication is needed.
Communication and bottlenecks on the other hand both introduce wait time which depends on the number of processors.
Differentiating between them relies on knowledge of the specific hardware of the system running the calculations.
This means how many cores are on a single chip, motherboard or node, which resources are shared between how many cores etc.\,.

For this interpretation to be meaningful, the CPU and wall times reported by \QE have to be accurate.
As an example for how errors could be overseen, when executing programs on multiple processors in parallel, \gls{cpu_time} is measured per processors.
This means some kind of information truncation is done when a single number (such as in \QE) is reported.
Whether this is taking the average over all processors, just reporting the time for a single processor or any other kind of truncation is unclear.

However, the notion of using the difference between \gls{wall_time} and \gls{cpu_time} for evaluating the quality of parallelization is supported by the user guide for one of the \QE modules \cite{noauthor_pwscf_nodate} (sec. 4.5), therefore it will also be used as a qualitative measure of good parallelization in this thesis.

\section{Hardware configuration of the PHYSnet cluster}\label{sec:hardware_physnet}

All calculations were run on a reserved subset of the \texttt{infinix} queue on the PHYSnet compute cluster with 20 nodes.
As of time of writing, the nodes in this queue are equipped with two Intel Xeon E5-2680 CPUs, as such providing 20 cores per node, 10 per chip and 200 processors in the whole queue.
The nodes are connected with an Infiniband FDR 4x network.

\end{document}
